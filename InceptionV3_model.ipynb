{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV3_model.ipynb",
      "provenance": [],
      "mount_file_id": "11xzMexfDnzgb5cDjic7IKeLxIh62t7YZ",
      "authorship_tag": "ABX9TyNRw7zzlA9i1kY83WRPGqO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BishalMondal/Crop-Disease-Detection/blob/main/InceptionV3_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZd4nhOyWJg1",
        "outputId": "52bc07cd-f603-49fc-df51-0369d6a447bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import os\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyyaml h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k99uxen_wl-O",
        "outputId": "075958c0-ac25-4f2c-89ea-924384725ffb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run only if mounting drive to be done\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2-Gc6zSWYap",
        "outputId": "ffba5453-d186-4313-8b6b-d30551daf495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")"
      ],
      "metadata": {
        "id": "s_oajueZXqJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTrainData(image_resize, batch_size_training):\n",
        "  train_generator = data_generator.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/training-data\",\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')\n",
        "  return train_generator"
      ],
      "metadata": {
        "id": "1gSOtQInXvYo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadValidationData(image_resize, batch_size_validation):\n",
        "  validation_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/MyDrive/validation-data',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_validation,\n",
        "    class_mode='categorical')\n",
        "  return validation_generator"
      ],
      "metadata": {
        "id": "gSMtxQpAX7bx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myCallback():\n",
        "  checkpoint_filepath = '/content/drive/MyDrive/tmp/checkpoint/'\n",
        "\n",
        "  #checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
        "\n",
        "  try:\n",
        "    os.mkdir(checkpoint_filepath)\n",
        "  except OSError as error:\n",
        "    print(error)    \n",
        "  model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "  return checkpoint_filepath, model_checkpoint_callback"
      ],
      "metadata": {
        "id": "qyYFOj3OYFCR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(InceptionV3(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))\n",
        "  \n",
        "  num_classes = 61\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  model.layers[0].trainable = False\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Un9EJkpPtLgR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epoch_num, image_resize, batch_size_training, batch_size_validation, model_checkpoint_callback, checkpoint_filepath):\n",
        "  num_classes = 61\n",
        "\n",
        "  #image_resize = 224\n",
        "\n",
        "  #batch_size_training = 64\n",
        "  #batch_size_validation = 64\n",
        "  train_generator = loadTrainData(image_resize, batch_size_training)\n",
        "  validation_generator = loadValidationData(image_resize, batch_size_validation)\n",
        "\n",
        "  model = get_model()\n",
        "\n",
        "  fit_history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=batch_size_training/4,\n",
        "    epochs=epoch_num,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=batch_size_validation/4,\n",
        "    callbacks=[model_checkpoint_callback]\n",
        "  )\n",
        "\n",
        "  #model.load_weights(checkpoint_filepath)\n",
        "  #!mkdir -p saved_model\n",
        "  model.save('saved_model/my_model')\n",
        "\n",
        "  return fit_history"
      ],
      "metadata": {
        "id": "gpS47uLsvHEP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G57en5r4JzjK",
        "outputId": "1a6f9318-766a-4008-8b72-8fddfc1df1de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath, model_checkpoint_callback = myCallback()\n",
        "os.listdir(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "N9r12Qe-d3pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461fb55a-45e1-4037-d636-f81b641720e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 17] File exists: '/content/drive/MyDrive/tmp/checkpoint/'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_history = train_model(20, 224, 64, 64, model_checkpoint_callback, checkpoint_filepath)"
      ],
      "metadata": {
        "id": "3ArCtaBSeBpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c33f15-b0e5-40da-d122-f9e0154350ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31751 images belonging to 61 classes.\n",
            "Found 4546 images belonging to 61 classes.\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 528s 34s/step - loss: 17.2072 - accuracy: 0.0713 - val_loss: 13.7332 - val_accuracy: 0.0811\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 440s 28s/step - loss: 11.0885 - accuracy: 0.1289 - val_loss: 10.3593 - val_accuracy: 0.1406\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 393s 25s/step - loss: 9.0805 - accuracy: 0.1717 - val_loss: 8.4138 - val_accuracy: 0.2207\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 367s 23s/step - loss: 7.8577 - accuracy: 0.2080 - val_loss: 7.5199 - val_accuracy: 0.2188\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 339s 21s/step - loss: 7.8436 - accuracy: 0.2246 - val_loss: 7.4073 - val_accuracy: 0.2178\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 302s 19s/step - loss: 7.9664 - accuracy: 0.2227 - val_loss: 6.9401 - val_accuracy: 0.2002\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 294s 19s/step - loss: 6.8546 - accuracy: 0.2480 - val_loss: 6.3782 - val_accuracy: 0.2373\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 267s 17s/step - loss: 6.2179 - accuracy: 0.2461 - val_loss: 6.8785 - val_accuracy: 0.2344\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 253s 16s/step - loss: 6.4110 - accuracy: 0.2588 - val_loss: 6.4897 - val_accuracy: 0.2705\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 241s 15s/step - loss: 5.7512 - accuracy: 0.2793 - val_loss: 6.7887 - val_accuracy: 0.2383\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 241s 15s/step - loss: 5.7550 - accuracy: 0.2881 - val_loss: 6.4881 - val_accuracy: 0.2314\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 215s 14s/step - loss: 5.7771 - accuracy: 0.2910 - val_loss: 5.3439 - val_accuracy: 0.2871\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 211s 13s/step - loss: 5.2036 - accuracy: 0.3232 - val_loss: 6.0050 - val_accuracy: 0.2744\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 207s 13s/step - loss: 5.5471 - accuracy: 0.3066 - val_loss: 5.4286 - val_accuracy: 0.3008\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 212s 13s/step - loss: 5.3986 - accuracy: 0.3145 - val_loss: 5.9696 - val_accuracy: 0.2842\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 203s 13s/step - loss: 5.2161 - accuracy: 0.3271 - val_loss: 5.7828 - val_accuracy: 0.3252\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 181s 11s/step - loss: 5.1861 - accuracy: 0.3135 - val_loss: 5.9046 - val_accuracy: 0.2979\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 178s 11s/step - loss: 4.9434 - accuracy: 0.3672 - val_loss: 5.4280 - val_accuracy: 0.3105\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 171s 11s/step - loss: 5.3881 - accuracy: 0.3086 - val_loss: 5.1645 - val_accuracy: 0.3174\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 170s 11s/step - loss: 5.1428 - accuracy: 0.3467 - val_loss: 5.9904 - val_accuracy: 0.2900\n",
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bVvy9b960Vxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}